{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc079d34-3fb5-4797-b384-dbb1cd8e7d11",
   "metadata": {},
   "source": [
    "## 1.线性回归demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99a6195e-7815-4e62-8fb9-ce618995da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "import torch.nn as nn\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.ndim  = ndim\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(ndim, 1)) \n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "    def forward(self, x):\n",
    "        # y= Wx + b \n",
    "        return x.mm(self.weight) + self.bias\n",
    "\n",
    "print(torch.__version__) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04c731e7-ecb8-41c9-8544-1d8b00a92e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3832],\n",
       "         [-0.8377],\n",
       "         [-1.4740],\n",
       "         [-1.6321]], grad_fn=<AddBackward0>),\n",
       " LinearModel())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearModel(5)\n",
    "x = torch.randn(4,5)  # batch 为4\n",
    "lm(x), lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec9efd6-bc53-4528-aba5-cf0f62f44cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'ndim',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee1647b-420f-4e95-a109-e7a10159b865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<generator object Module.named_parameters at 0x0000011E607A3F40>,\n",
       " [('weight',\n",
       "   Parameter containing:\n",
       "   tensor([[-0.8593],\n",
       "           [ 1.0687],\n",
       "           [-0.0311],\n",
       "           [ 0.9709],\n",
       "           [ 0.5852]], requires_grad=True)),\n",
       "  ('bias',\n",
       "   Parameter containing:\n",
       "   tensor([0.3763], requires_grad=True))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.named_parameters(), list(lm.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be5cf919-b216-4aae-aaec-aafe7c0ed37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.8593],\n",
       "          [ 1.0687],\n",
       "          [-0.0311],\n",
       "          [ 0.9709],\n",
       "          [ 0.5852]], device='cuda:0', requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.3763], device='cuda:0', requires_grad=True))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.cuda() # 将 模型参数转移到GPU 上，注意 device='cuda:0'\n",
    "list(lm.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b223c21-052b-4bc5-8573-28edbbb115dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.half()  # 转换模型参数为半精浮点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b4fb376-35e5-49b7-8f63-42064e5ed36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.8594],\n",
       "          [ 1.0684],\n",
       "          [-0.0311],\n",
       "          [ 0.9707],\n",
       "          [ 0.5854]], device='cuda:0', dtype=torch.float16, requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.3762], device='cuda:0', dtype=torch.float16, requires_grad=True))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lm.named_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24d83228-c01a-4ddc-a0d3-1f3bcb2df2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.8594],\n",
       "                      [ 1.0684],\n",
       "                      [-0.0311],\n",
       "                      [ 0.9707],\n",
       "                      [ 0.5854]], device='cuda:0', dtype=torch.float16)),\n",
       "             ('bias', tensor([0.3762], device='cuda:0', dtype=torch.float16))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7d4db-2c3b-4140-8335-f55499990345",
   "metadata": {},
   "source": [
    "## 2.  求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4cb00b-04b2-448f-b070-40127484cecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9360, -0.4411, -0.7872],\n",
       "        [ 0.1219, -0.3136,  0.4278],\n",
       "        [-1.1784,  0.8631, -0.1838]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  \n",
    "import numpy as np\n",
    "\n",
    "t1 = torch.randn(3,3, requires_grad=True)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4692a945-5d9c-425d-8fa3-6d286a0571e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1539, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2= t1.pow(2).sum()  # t2 = t1^2\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80bbee8-3b61-41eb-95a8-0ffc8b62557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3276b57-7e77-4ae2-aa9b-c2d7fb4e0d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8719, -0.8823, -1.5745],\n",
       "        [ 0.2438, -0.6271,  0.8556],\n",
       "        [-2.3568,  1.7261, -0.3677]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80ade8cf-278b-4b69-9e9b-714eb0660a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7438, -1.7646, -3.1489],\n",
       "        [ 0.4875, -1.2543,  1.7112],\n",
       "        [-4.7136,  3.4523, -0.7354]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2= t1.pow(2).sum()  # t2 = t1^2\n",
    "t2.backward()\n",
    "t1.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4550b98-d498-4359-81f6-b62445150eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.grad.zero_() # 梯度清零\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58287458-7696-4f40-b9de-5e3951a8d685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5256564e-518c-4f0d-8dc4-c6d1459e6379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "t2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c3a7f-5e4c-40d8-9bbe-bb760edc0adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
